{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing header files**"
      ],
      "metadata": {
        "id": "tMUO8QB_cPAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "dy-WGnDXfvOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**assignment2**"
      ],
      "metadata": {
        "id": "UtRwJRWaeVD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAMES\n",
        "\n",
        "1.Ankit Raj 2211cs03\n",
        "2.mukesh barpete 2211cs11\n",
        "3.vickey jeshwani 2211cs19"
      ],
      "metadata": {
        "id": "qJUBTwbDgeHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import random"
      ],
      "metadata": {
        "id": "znX9a6THcb11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading data set from txt file line by line**"
      ],
      "metadata": {
        "id": "Mxpe9KaNclhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (r\"/content/NER-Dataset-Train.txt\") as f:\n",
        "    lines=f.readlines()"
      ],
      "metadata": {
        "id": "XOB7zlC9cn8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"\"\"SPlitting data into pairs of word-Tag form **"
      ],
      "metadata": {
        "id": "__chMm8Lcx6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines[0]\n",
        "pairs=[[]]\n",
        "for i in lines:\n",
        "    if i=='\\n':\n",
        "        pairs.append([])\n",
        "        continue\n",
        "    strip=i.strip('\\n').split('\\t')\n",
        "    pairs[-1].append(strip)\n",
        "\n",
        "pairs[0]"
      ],
      "metadata": {
        "id": "voX8HSXtc1q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"Create an HMM class having 4 methods.\n",
        "\n",
        "1.   train\n",
        "2.   viterbi\n",
        "3.   test\n",
        "4.   fivefold"
      ],
      "metadata": {
        "id": "AU5bB47kc6sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HMM:\n",
        "    def __init__(self,value,no_hidden_states=3):\n",
        "        self.value=value\n",
        "        self.no_hidden_states=no_hidden_states\n",
        "        self.pi=np.ones((self.no_hidden_states),dtype='float64') # pi initial probabilities\n",
        "        self.TMatrix=np.ones((self.no_hidden_states,self.no_hidden_states),dtype='float64') # Transision matrix for bigram\n",
        "        self.UniqueWords=set()\n",
        "        self.train_data=None\n",
        "        self.UniqueTags=set()\n",
        "        self.UniqueTags_to_numbers=dict()\n",
        "        self.UniqueWords_to_numbers=dict()\n",
        "        self.numbers_to_tags=dict()\n",
        "\n",
        "    def train(self,train_data):\n",
        "        train_data=[x for x in train_data if len(x)>3]\n",
        "        for i in train_data:\n",
        "            for ind,[j,k] in enumerate(i):\n",
        "                self.UniqueTags.add(k)\n",
        "                self.UniqueWords.add(j)\n",
        "        self.UniqueWords.add('UNK') # Adding Unkown word\n",
        "        self.EMatrix=np.ones((self.no_hidden_states,len(self.UniqueWords)),dtype='float64') # Emmision matrix\n",
        "        self.UniqueWords,self.UniqueTags=list(self.UniqueWords),list(self.UniqueTags)\n",
        "        self.UniqueTags_to_numbers={i:ind for ind,i in enumerate(self.UniqueTags)}\n",
        "        self.UniqueWords_to_numbers={i:ind for ind,i in enumerate(self.UniqueWords)}\n",
        "        self.numbers_to_tags={j:i for i,j in self.UniqueTags_to_numbers.items()}\n",
        "        previous=None #Three pointers used to keep track of the pre  , current and next word\n",
        "        next=None\n",
        "        for i in train_data:\n",
        "            for ind,[j,k] in enumerate(i):\n",
        "                if ind==0:\n",
        "                    self.pi[self.UniqueTags_to_numbers[k]]+=1\n",
        "                    previous=k\n",
        "                    continue\n",
        "                next=k\n",
        "                self.TMatrix[self.UniqueTags_to_numbers[previous]][self.UniqueTags_to_numbers[next]]+=1\n",
        "                previous=next\n",
        "                self.EMatrix[self.UniqueTags_to_numbers[k]][self.UniqueWords_to_numbers[j]]+=1\n",
        "        self.pi=self.pi/self.pi.sum()\n",
        "        self.TMatrix=self.TMatrix/self.TMatrix.sum(axis=1)[:,np.newaxis]\n",
        "        self.EMatrix=self.EMatrix/self.EMatrix.sum(axis=1)[:,np.newaxis]\n",
        "\n",
        "\n",
        "    def viterbi(self,words):\n",
        "        matrix=np.zeros((len(words),self.no_hidden_states))-1e+4\n",
        "        for j in words:\n",
        "            if j not in self.UniqueWords_to_numbers:\n",
        "                self.UniqueWords_to_numbers[j]=1\n",
        "        matrix[0]=np.log(self.pi)*self.value+np.log(self.EMatrix[:,self.UniqueWords_to_numbers[words[0]]])\n",
        "        for i in range(1,len(words)):\n",
        "            for j in range(self.no_hidden_states):\n",
        "                for k in range(self.no_hidden_states):\n",
        "                    matrix[i][j]=max(matrix[i-1][k]*self.value+np.log(self.TMatrix[k][j])*\\\n",
        "                                     self.value+np.log(self.EMatrix[j][self.UniqueWords_to_numbers[words[i]]]),\n",
        "                                                matrix[i][j] )\n",
        "        resultseq=[]\n",
        "        for i in np.argmax(matrix,axis=1):\n",
        "            resultseq.append(self.numbers_to_tags[i])\n",
        "        return resultseq\n",
        "\n",
        "\n",
        "    def test(self,test_data):\n",
        "        return_sequences=[]\n",
        "        y_pred=[]\n",
        "        y_true=[]\n",
        "        for i in test_data:\n",
        "            i=np.array(i)\n",
        "            y_true+=[x for x in i[:,1]]\n",
        "            y_pred+=self.viterbi(i[:,0])\n",
        "        acc=np.count_nonzero(np.array(y_true)==np.array(y_pred))/len(y_true)\n",
        "        return *precision_recall_fscore_support(y_pred,y_true,average='macro',zero_division=0),acc\n",
        "\n",
        "\n",
        "    def FiveFold(self,data):\n",
        "        index=int(len(data)/5)\n",
        "        _=[\"Unigram Model\",\"Bigram Model\"]\n",
        "        print(f\"{'*'*60}\\nFor {_[self.value]}\\n\")\n",
        "        for i in range(5):\n",
        "            train=data[0:index*i]+data[index*(i+1):]\n",
        "            test=data[index*i:index*(i+1)]\n",
        "            self.train(train)\n",
        "            test_result=self.test(test)\n",
        "            print(f\"Precision {i+1}th fold=\"+str(round(test_result[0],2)),\n",
        "                  f\"\\tRecall {i+1}th fold=\"+str(round(test_result[1],2)),\n",
        "                  f\"\\tF1_score {i+1}th fold=\"+str(round(test_result[2],2)),\n",
        "                  f\"\\tAccuracy {i+1}th fold=\"+str(round(test_result[-1],2)),\"\\n\")\n",
        "            self.__init__(self.value,self.no_hidden_states)"
      ],
      "metadata": {
        "id": "gSfbNokYdC1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMM for Bigram Model"
      ],
      "metadata": {
        "id": "pO6hW6tzddSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_bigram=HMM(1)\n",
        "hmm_bigram.FiveFold(pairs)\n",
        "\n",
        "hmm_bigram.train(pairs)\n",
        "hmm_bigram.TMatrix\n",
        "\n",
        "hmm_bigram.EMatrix\n"
      ],
      "metadata": {
        "id": "1h7B0zUwdXjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"HMM for Unigram Model"
      ],
      "metadata": {
        "id": "q4lkNp6xdrdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_unigram=HMM(0)\n",
        "hmm_unigram.FiveFold(pairs)"
      ],
      "metadata": {
        "id": "pTvYZVjgdolg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"Transmission Probabilities\"\"\""
      ],
      "metadata": {
        "id": "_ykdX8cTd0zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_unigram.train(pairs)\n"
      ],
      "metadata": {
        "id": "RfdhWiVZd2Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"Emission Probability\"\""
      ],
      "metadata": {
        "id": "FqUFx4uWeARF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_unigram.EMatrix"
      ],
      "metadata": {
        "id": "P4HrD36geD0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MLCW6aJnjui7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"# RNN : Recurrent Neural Network\n",
        "An RNN is a type of neural network that can process sequential data by maintaining a hidden state, which is updated at each time step using the input and the previous hidden state. The main features of an RNN are:\n",
        "\n",
        "    Hidden state: The RNN maintains a hidden state that is updated at each time step and serves as a memory of the previous inputs.\n",
        "\n",
        "    Time dependency: The output of the RNN at each time step depends not only on the current input but also on the previous inputs, through the hidden state.\n",
        "\n",
        "    Parameter sharing: The same set of weights is used at each time step, which allows the network to learn to process sequential data of varying length.\n",
        "\n",
        "Now, let's move on to the architecture of the RNN for NER tagging. The input to the network is a sequence of tokens, where each token is represented by a vector of features (e.g., word embedding, part-of-speech tag). The output of the network is a sequence of tags, where each tag represents the named entity label of the corresponding token.\n",
        "\n",
        "![2020-02-21_difference-between-cnn-rnn-1.webp](data:image/webp;base64,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)\n",
        "\n",
        "The architecture of the RNN for NER tagging is shown below:\n",
        "\n",
        "      x1         x2         ...         xn\n",
        "      |          |                     |\n",
        "      V          V                     V\n",
        "    [embedding] [embedding]  ...  [embedding]\n",
        "      |          |                     |\n",
        "      V          V                     V\n",
        "    [RNN cell] [RNN cell]   ...  [RNN cell]\n",
        "      |          |                     |\n",
        "      V          V                     V\n",
        "    [dense]    [dense]      ...  [dense]\n",
        "      |          |                     |\n",
        "      V          V                     V\n",
        "      y1         y2         ...         yn\n",
        "\n",
        "The input sequence of tokens is first passed through an embedding layer that converts each token into a vector of features. The embedded sequence is then fed into a sequence of RNN cells, each of which updates the hidden state based on the input and the previous hidden state. The output of each RNN cell is passed through a dense layer that maps the hidden state to a vector of scores for each possible tag. Finally, the sequence of tag scores is transformed into a sequence of tags using a softmax activation function.\n",
        "\n",
        "The justification for this architecture is that it can effectively capture the sequential nature of the NER tagging task, as well as the context-dependent relationships between tokens and their corresponding named entity labels. By maintaining a hidden state and parameter sharing, the RNN can learn to process sequences of varying length and capture long-term dependencies between tokens. Additionally, the use of a dense layer allows the network to map the hidden state to a vector of scores for each tag, which can be used to make tag predictions for each token in the sequence.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vV7y0ojrjpR1"
      }
    }
  ]
}